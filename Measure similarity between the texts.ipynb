{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the csv file.\n",
    "import csv\n",
    "file = open(\"Requirements Dataset.csv\")\n",
    "reader = csv.reader(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 463), ('the', 285), (\"''\", 224), ('.', 173), ('to', 99), ('of', 95), ('and', 93), ('should', 86), ('``', 84), ('be', 72)]\n"
     ]
    }
   ],
   "source": [
    "#remove the stop words.\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def get_tokens():\n",
    "   with open('Requirements Dataset.csv', 'r') as shakes:\n",
    "    text = shakes.read()\n",
    "    lowers = text.lower()\n",
    "    #remove the punctuation using the character deletion step of translate\n",
    "    no_punctuation = lowers.translate(string.punctuation)\n",
    "    tokens = nltk.word_tokenize(no_punctuation)\n",
    "    return tokens\n",
    "\n",
    "tokens = get_tokens()\n",
    "count = Counter(tokens)\n",
    "print (count.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 463), (\"''\", 224), ('.', 173), ('``', 84), ('data', 54), (':', 43), ('python', 34), ('r', 34), ('algorithm', 33), ('dataset', 30), ('system', 25), (')', 25), ('using', 23), ('user', 22), ('machine', 22), ('datasets', 20), ('performance', 19), ('results', 19), ('able', 18), ('accuracy', 18), ('chatbot', 18), ('requirements', 17), ('(', 17), ('training', 16), ('used', 16), ('detection', 14), ('result', 14), ('1', 14), ('matlab', 13), ('given', 13), ('prediction', 12), ('\\uf0b7', 12), ('application', 12), ('time', 12), ('’', 12), ('one', 11), ('risk', 11), ('features', 11), ('recognition', 10), ('based', 10), ('price', 10), ('text', 9), ('–', 9), ('different', 9), ('positive', 9), ('output', 9), ('predict', 9), ('processing', 9), ('bot', 8), ('classification', 8), ('-', 8), ('random', 8), ('accurate', 8), ('algorithms', 8), ('programming', 8), ('learning', 8), ('error', 8), ('neural', 8), ('language', 8), ('twitter', 8), ('patterns', 8), ('information', 8), ('analysis', 8), ('1.', 8), ('program', 8), ('images', 8), ('3', 8), ('set', 7), ('obtained', 7), ('forest', 7), ('classify', 7), ('studio', 7), ('android', 7), ('input', 7), ('negative', 7), ('network', 7), ('bayes', 7), ('house', 7), ('processor', 7), ('i.e', 7), ('use', 7), ('%', 7), ('person', 7), ('actual', 7), ('feature', 7), ('anomaly', 7), ('hence', 7), ('image', 7), ('regression', 6), ('security', 6), ('digits', 6), ('naive', 6), ('number', 6), ('errors', 6), ('area', 6), ('knn', 6), ('quickly', 6), ('movie', 6), ('category', 6), ('model', 6)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokens = get_tokens()\n",
    "filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "count = Counter(filtered)\n",
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(',', 463), (\"''\", 224), ('.', 173), ('``', 84), ('data', 54), ('dataset', 50), ('use', 48), (':', 43), ('algorithm', 41), ('python', 34), ('r', 34), ('result', 33), ('requir', 29), ('predict', 28), ('user', 27), ('system', 25), (')', 25), ('train', 23), ('detect', 23), ('machin', 22), ('perform', 22), ('accuraci', 18), ('featur', 18), ('chatbot', 18), ('abl', 18), ('classifi', 17), ('(', 17), ('program', 16), ('imag', 15), ('1', 14), ('error', 14), ('matlab', 13), ('given', 13), ('accur', 12), ('\\uf0b7', 12), ('applic', 12), ('time', 12), ('process', 12), ('’', 12), ('model', 12), ('network', 11), ('one', 11), ('risk', 11), ('price', 11), ('set', 10), ('base', 10), ('movi', 10), ('analyz', 10), ('pattern', 10), ('inform', 10), ('learn', 10), ('recognit', 10), ('text', 9), ('output', 9), ('digit', 9), ('–', 9), ('differ', 9), ('test', 9), ('classif', 9), ('posit', 9), ('bot', 8), ('-', 8), ('analysi', 8), ('random', 8), ('twitter', 8), ('neural', 8), ('languag', 8), ('1.', 8), ('work', 8), ('obtain', 8), ('3', 8), ('review', 8), ('anomali', 7), ('packag', 7), ('forest', 7), ('studio', 7), ('hous', 7), ('android', 7), ('input', 7), ('bay', 7), ('henc', 7), ('processor', 7), ('develop', 7), ('i.e', 7), ('%', 7), ('person', 7), ('actual', 7), ('select', 7), ('face', 7), ('neg', 7), ('indic', 6), ('number', 6), ('regress', 6), ('avail', 6), ('area', 6), ('plot', 6), ('knn', 6), ('show', 6), ('repli', 6), ('categori', 6)]\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import *\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = stem_tokens(filtered, stemmer)\n",
    "count = Counter(stemmed)\n",
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path = '/home/pratima/Desktop'\n",
    "token_dict = {}\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens, stemmer):\n",
    "    stemmed = []\n",
    "    for item in tokens:\n",
    "        stemmed.append(stemmer.stem(item))\n",
    "    return stemmed\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = stem_tokens(tokens, stemmer)\n",
    "    return stems\n",
    "\n",
    "for subdir, dirs, files in os.walk(path):\n",
    "    for file in files:\n",
    "        file_path = subdir + os.path.sep + file\n",
    "        measure= open(\file_path\, 'r')\n",
    "        text = measure.read()\n",
    "        lowers = text.lower()\n",
    "        remove_punctuation = lowers.translate(string.punctuation)\n",
    "        token_dict[file] = remove_punctuation\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "searchText = input('Enter Text')\n",
    "response = tfidf.transform([searchText])\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print(feature_names[col], ' - ', response[0, col])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
